%04-evaluation.tex
\chapter{Evaluation}
\label{ch:eval}

In this chapter, we evaluate the correctness and the performance of our MLIR-based simulator using the same test set used in the LLHD paper \cite{Schuiki2020}, including a full \textit{Snitch} processor \cite{Zaruba2020}. We use \texttt{moore} version $0.6.0$ as the reference compiler to get the SystemVerilog test set in LLHD (with no additional flags active during compilation), paired with Martin Erhart's MLIR writer, to convert the original LLHD syntax to the MLIR syntax. We note that the version of \texttt{moore} we're using doesn't correspond to the latest available version, but it is the most tested, and we can guarantee that the LLHD output it generates is well-formed. When using the latest version ($0.10.0$ at the time of writing), we noticed some discrepancies that could be either due to edge-cases not covered in our simulator or bugs introduced in \texttt{moore} after some major refactorings. We note that in \texttt{moore} version $0.11.0$, released after we generated our evaluation, those discrepancies seem to have been lifted, though a more accurate inspection would be required. All the examples of the test set, as well as the scripts used to generate the evaluations, can be found on our public testing GitHub repo\footnote{\url{github.com/rodonisi/llhd-extras}}.

%---------------------------------------------------------------------------------------------------

\section{Simulator Correctness}
\label{sec:correctness}
We test for correctness by comparing the trace generated by our simulator for the top-level signals (using the variant 4. of the trace format, described in Section \ref{sec:trace}) against the one generated by Xilinx's Vivado's simulator \cite{vivado}, once again only keeping the top-level signals of the design (\ie, the test-bench).

To obtain a reliable comparison, we wrote a simple python script to translate the VCD trace generated by Vivado to our textual format (using the \textit{vcdvcd} \cite{vcdvcd} module to parse the input VCD file). We then proceed by generating a diff (using the \texttt{diff} command-line tool) of the full trace generated by Vivado, against the full trace we obtain with our simulator.

We remark that since we cannot currently save the port names for the various instance's signals, a comparison of the traces below the top-level becomes a non-trivial task. For the top-level though, where all the signal names are dictated by the original signal names, rather than the port names, we can trivially perform a comparison. Furthermore, by treating the underlying circuit as a “black box“ and observing its output signals, we can still infer the correctness of our simulation, as those signals depend on the correct behavior of the circuit.

We observe that by using LLHD, we are simulating slightly different designs from the original SystemVerilog implementations, due to the  lack of a logical type in the current state of LLHD, which is vastly used in SystemVerilog designs. We thus expect slight variations in the simulation traces, but only in cases where a logic type takes either a “don't care“ value (\textit{X}) or a high-impedance value (\textit{Z}). An example can be seen in Listing \ref{listing:diff-lfsr}.

\begin{listing}[ht]
  \lstset{
    escapeinside=||,
    basicstyle=\footnotesize\ttfamily,
    stepnumber=1
  }
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  |\textcolor{red}{lfsr\_16bit\_tb/clk\_i  x}|
  |\textcolor{red}{lfsr\_16bit\_tb/en\_i  x}|
  |\textcolor{red}{lfsr\_16bit\_tb/out\_o  x}|
  lfsr_16bit_tb/rst_ni  0x01
1000ps
  lfsr_16bit_tb/out_o  0x0001
  lfsr_16bit_tb/rst_ni  0x00
2000ps
  lfsr_16bit_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  |\textcolor{ForestGreen}{lfsr\_16bit\_tb/clk\_i  0x00}|
  |\textcolor{ForestGreen}{lfsr\_16bit\_tb/en\_i  0x00}|
  |\textcolor{ForestGreen}{lfsr\_16bit\_tb/out\_o  0x0000}|
  lfsr_16bit_tb/rst_ni  0x01
1000ps
  lfsr_16bit_tb/out_o  0x0001
  lfsr_16bit_tb/rst_ni  0x00
2000ps
  lfsr_16bit_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \caption[Side-by-side zoom-in of the LFSR example's diff.]{Side-by-side zoom-in of the LFSR example's diff. Here we can see the case where a \texttt{X} value in Vivado's trace (left) is not reflected in our trace (right), as it cannot currently be represented in LLHD. Note that this excerpt contains all the differences for this example.}
  \label{listing:diff-lfsr}
\end{listing}

A further case where we expect differences is where we don't observe a change due to a previous \texttt{X} or \texttt{Z} value. Listing \ref{listing:diff-lzc} shows an example where the change at $1000ps$ for signal \textit{cnt\_o} is not reflected in our trace, as the signal is already on that value (at line $3$).


\begin{listing}[ht]
  \lstset{
    escapeinside=||,
    basicstyle=\footnotesize\ttfamily,
    stepnumber=1
  }
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  |\textcolor{red}{lzc\_tb/clk\_i  x}|
  |\textcolor{red}{lzc\_tb/cnt\_o  x}|
  |\textcolor{red}{lzc\_tb/empty\_o  x}|
  |\textcolor{red}{lzc\_tb/in\_i  x}|
  lzc_tb/rst_ni  0x01
1000ps
  |\textcolor{red}{lzc\_tb/cnt\_o  0x0f}|
  lzc_tb/empty_o  0x00
  lzc_tb/in_i  0x0001
  lzc_tb/rst_ni  0x00
2000ps
  lzc_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  |\textcolor{ForestGreen}{lzc\_tb/clk\_i  0x00}|
  |\textcolor{ForestGreen}{lzc\_tb/cnt\_o  0x0f}|
  |\textcolor{ForestGreen}{lzc\_tb/empty\_o  0x01}|
  |\textcolor{ForestGreen}{lzc\_tb/in\_i  0x0000}|
  lzc_tb/rst_ni  0x01
1000ps

  lzc_tb/empty_o  0x00
  lzc_tb/in_i  0x0001
  lzc_tb/rst_ni  0x00
2000ps
  lzc_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \caption[Side-by-side zoom-in of the LZC example's diff.]{Side-by-side zoom-in of the LZC example's diff. Here we can see the case where an update from an \texttt{X} value to some other value is not reflected in our trace, if the signal is already carrying the new value in the LLHD model. Note that this excerpt contains all the differences for this example.}
  \label{listing:diff-lzc}
\end{listing}

The only exception not falling under those cases is in the \textit{Stream Delayer} example, where we can see a mismatch in the value initialization not caused by \texttt{X} values, as shown in Listing \ref{listing:diff-sd}. By inspecting our full trace, we observe that we initialize those signals to the correct values, but they are driven at a time sub-step of the initial real-time step ($0$ps). We conclude that Vivado is merging the initial time sub-steps into the next step, rather than the $0$ps step, as we do. We believe our approach to be correct, since treating the initial step as a special case makes the simulation trace less consistent. An additional toggle to enable this behavior, though, could be added to minimize the discrepancies between the simulation traces if what we believe to be Vivado's approach results in being a common practice in well-established simulators.

\begin{listing}[ht]
  \lstset{
    escapeinside=||,
    basicstyle=\footnotesize\ttfamily,
    stepnumber=1
  }
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  stream_delay_tb/clk_i  x
  [...]
  |\textcolor{red}{stream\_delay\_tb/ready\_i  0x00}|
  |\textcolor{red}{stream\_delay\_tb/ready\_o  0x00}|
  stream_delay_tb/rst_ni  0x01
  stream_delay_tb/valid_i  0x01
  |\textcolor{red}{stream\_delay\_tb/valid\_o  0x00}|
1000ps
  |\textcolor{red}{stream\_delay\_tb/ready\_i  0x01}|
  |\textcolor{red}{stream\_delay\_tb/ready\_o  0x01}|
  stream_delay_tb/rst_ni  0x00
  |\textcolor{red}{stream\_delay\_tb/valid\_o  0x01}|
2000ps
  stream_delay_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  stream_delay_tb/clk_i  0x00
  [...]
  |\textcolor{ForestGreen}{stream\_delay\_tb/ready\_i  0x01}|
  |\textcolor{ForestGreen}{stream\_delay\_tb/ready\_o  0x01}|
  stream_delay_tb/rst_ni  0x01
  stream_delay_tb/valid_i  0x01
  |\textcolor{ForestGreen}{stream\_delay\_tb/valid\_o  0x01}|
1000ps


  stream_delay_tb/rst_ni  0x00

2000ps
  stream_delay_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \caption[Side-by-side zoom-in of the Stream Delayer example's diff.]{Side-by-side zoom-in of the Stream Delayer example's diff. Here we can see the case where we have an initialization mismatch. Some signal changes have been omitted and replaced by a [...] designation for the sake of readability. Note that this excerpt contains all the differences for this example.}
  \label{listing:diff-sd}
\end{listing}

Considering those cases as exceptions, we can assert that we are generating equivalent (though not equal) simulation traces to Vivado's simulator for our test set. Most notably, for the \textit{Graycode} example, we can generate a $100\%$ matching trace. The \textit{Snitch} processor on the other side is the most complex example, and the one with most differences, but all of them fall under the cases described above. A full listing of the raw \texttt{diff}-generated differentials for all traces can be found in Appendix \ref{app:diffs}.

%---------------------------------------------------------------------------------------------------

\section{Performance Evaluation}
We evaluate the performance of our simulator by averaging the timing results over $10$ different runs for each design, in order to avoid major variations. We also disable the trace generation during performance evaluation. The rationale here is that we want to measure the performance of the actual simulation, without the bias introduced by trace processing and possible I/O bounds. For example, the processing required to generate the trace format used in Section \ref{sec:correctness} sometimes requires alone more than $3$ times the time of the actual simulation in the current implementation. We also consider that a user might not require generating a signal trace dump, but might rather be more interested in testing whether a test-bench can simulate without errors or triggering assertions, though we note that such features are not yet implemented in LLHD.

All the reported run-times have been generated by running our simulator in release mode on a Linux machine equipped with an i5 $750$ processor running at $2.67Ghz$, and $8GB$ of RAM. Each series of reports is also generated after a fresh restart. Both the simulator and MLIR/LLVM have been built using CMake's default \textit{release} mode, and LLVM IR code generation uses the (default) “O2“ flag.

\begin{table}
  \centering
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccc}
      \toprule
      \textbf{Design} & \textbf{MLIR Simulator [s]} & \begin{tabular}[c]{@{}l@{}}\textbf{LLHD Reference }\\\textbf{Simulator [s]**}\end{tabular} & \textbf{LLHD Blaze}~ \textbf{[s]} & \textbf{Vivado [s]} \\
      \toprule
      CDC (Gray)      & 421.95                      & 4894.94                   & 16.56                             & 12.00               \\
      CDC (strobe)    & 622.60                      & 5132.95                   & 16.13                             & 16.10               \\
      FIFO Queue      & 140.73                      & 2324.60                   & 11.77                             & 5.30                \\
      FIR Filter      & 745.64                      & 14807.50                  & 18.00                             & 33.10               \\
      Graycode        & 223.71                      & 14628.93                  & 9.79                              & 23.00               \\
      LFSR            & 636.52                      & 7080.40                   & 25.20                             & 37.20               \\
      LZC             & 156.65                      & 31207.36                  & 5.87                              & 8.00                \\
      RR Arbiter      & 1065.21                     & 84072.45                  & 17.55                             & 75.50               \\
      Snitch          & 384.24                      & 6721.92                   & 48.55                             & 4.80                \\
      Stream Delayer  & 139.72                      & 1136.31                   & 8.74                              & 2.85                \\
      \bottomrule
    \end{tabular}
  }

  \caption[Comparison of simulation timings for our first simulator prototype.]{Comparison of the time required to simulate the design of our test set on various simulators. The \texttt{MLIR Simulator} reports the timings obtained by the first implementation of our MLIR-based simulator. All time values are reported in seconds. \\ ** Due to the amount of time required by the LLHD reference simulator, the timings are reported for only one run.}
  \label{tab:timings_compare}
\end{table}

Table \ref{tab:timings_compare} lists the average timings we obtain with our first prototype of the simulator that can run all the examples from our test set. As a reference, we compare the performance we obtain against the LLHD reference simulator, the LLHD Blaze simulator, and the Vivado simulator. A full and raw listing of the timings results used to generate this evaluation can be found in Appendix \ref{app:timings}.

We observe that our simulator is nowhere near obtaining competitive performance when compared against both a commercial simulator and LLHD Blaze (which implements a very similar approach to ours). In particular, our performance is on average $31$ times slower than Vivado's simulator.

In table \ref{tab:time_detail0} we explore a more detailed view of the time spent in various stages of the simulation. Once again the listed values are averaged over multiple executions of the simulator, and full listings for each example can be found in Appendix \ref{app:timings}.

\begin{table}[ht]
  \centering
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccccc}
      \toprule
      \textbf{Design} & \textbf{Total [s]} & \begin{tabular}[c]{@{}l@{}}\textbf{Init}\\\textbf{phase}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Allocation}\\\textbf{phase}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Signals}\\\textbf{update}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Instances}\\\textbf{execution}\end{tabular} & \textbf{Remainder} \\
      \toprule
      CDC (Gray)      & 421.95             & 0\%                       & 0\%                       & 12.92\%                    & 71.48\%                    & 15.61\%            \\
      CDC (strobe)    & 622.60             & 0\%                       & 0\%                       & 10.37\%                    & 75.26\%                    & 14.37\%            \\
      FIFO Queue      & 140.73             & 0\%                       & 0\%                       & 12.22\%                    & 72.41\%                    & 15.37\%            \\
      FIR Filter      & 745.64             & 0\%                       & 0\%                       & 10.09\%                    & 77.96\%                    & 11.95\%            \\
      Graycode        & 223.71             & 0\%                       & 0\%                       & 7.82\%                     & 82.25\%                    & 9.93\%             \\
      LFSR            & 636.52             & 0\%                       & 0\%                       & 10.04\%                    & 79.86\%                    & 10.10\%            \\
      LZC             & 156.65             & 0\%                       & 0\%                       & 10.92\%                    & 76.86\%                    & 12.23\%            \\
      RR Arbiter      & 1065.21            & 0\%                       & 0\%                       & 13.16\%                    & 67.21\%                    & 19.63\%            \\
      Snitch          & 384.24             & 0\%                       & 0\%                       & 11.69\%                    & 67.30\%                    & 20.75\%            \\
      Stream Delayer  & 139.72             & 0\%                       & 0\%                       & 10.38\%                    & 73.07\%                    & 16.55\%            \\
      \bottomrule
    \end{tabular}
  }
  \caption[Detailed performance evaluation of the MLIR-based simulator]{Detailed performance evaluation of the MLIR-based simulator. The \textit{Total} column indicates the total time required to simulate the design from start to end, as reported by the \texttt{time} command-line tool. The remaining columns report the overall percentage of the time spent in that simulation phase. The \textit{Init phase} column includes initializing the state structures, converting the LLHD module to the LLVM dialect, and initializing the JIT compiler. The \textit{Allocation phase} includes executing the generated initialization function. The \textit{Signals update} column includes processing and applying the scheduled updates. The \textit{remainder} reports the percentage of time spent in stages we do not track, including constructing and destroying the simulator, iterating over, and popping the event queue.}
  \label{tab:time_detail0}
\end{table}

We recognize our first prototype prioritized correctness over performance, and as such many of the used data structures and approaches are far from being optimal. Though, we can see an interesting result in the initialization times, where we see the negligible impact of MLIR applying the LLHD-to-LLVM conversion pass (part of the \textit{init phase} metric), as well as the execution of the generated initialization function (\textit{allocation phase} metric). In both cases, the total time required rounds down to $0\%$ during the formatting of the results. We also see that a great majority of the required simulation time resides in the execution of the various instances. Due to time constraints, we were not able to bring major changes targeted at improving performance, but we observe that some performance optimizations include some low-hanging fruits.

By inspecting the simulator execution with a profiling tool, \texttt{perf}, we get a clearer picture of where we are losing performance. We see that most of the time is spent allocating and deallocating memory the simulator queue and state-keeping structures. Also, we note that the execution of our lowered LLHD code takes a negligible amount of time (around $1$-$2\%$).

Given the profiling results, we applied the following changes, achieving the timing results listed in Table \ref{tab:time_detail1}:

\begin{enumerate}
  \item An overall cleanup of the code and replacement of basic data structures with more efficient implementations, \eg, using LLVM \texttt{SmallVector} instead of standard vectors (where fit).
  \item Initially, we were executing the units through the JIT engine's \texttt{invoke} function. Looking up the compiled function pointers through the JIT compiler turns out to be very expensive, so we switched to storing the compiled function pointers in the instance structures, making them readily available when needed.
  \item We used to map instances to the respective structures by name, requiring us to perform lots of string allocations and comparisons when iterating over the wakeup queue. We thus changed to an integer indexing, more similar to what we were already doing with signals, dramatically cutting down on strings usage.
  \item We observe that the number of events present at the same time in the event queue is usually fairly small, usually between $2$ and $6$. Given this, we can change the event queue to be a simple, unsorted vector, where we search for the earliest slot in each cycle instead of sorting the queue for each drive. Furthermore, this allows us to reuse the already allocated slots instead of popping and destroying them, significantly cutting down the allocation/deallocation times. We applied the same approach to the slot change buffers, replacing the signal-to-changes map structure with a vector of index-change pairs, and reusing the buffers instead of allocating new ones each time.
\end{enumerate}

\begin{table}[ht]
  \centering
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccccc}
      \toprule
      \textbf{Design} & \textbf{Total [s]} & \begin{tabular}[c]{@{}l@{}}\textbf{Init}\\\textbf{phase}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Allocation}\\\textbf{phase}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Signals}\\\textbf{update}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Instances}\\\textbf{execution}\end{tabular} & \textbf{Remainder} \\
      \toprule
      CDC (Gray)      & 32.34              & 0\%                        & 0\%                        & 27.83\%                    & 56.90\%                    & 15.27\%            \\
      CDC (strobe)    & 41.70              & 0\%                        & 0\%                        & 28.51\%                    & 57.56\%                    & 13.93\%            \\
      FIFO Queue      & 12.33              & 0\%                        & 0\%                        & 30.01\%                    & 48.67\%                    & 21.32\%            \\
      FIR Filter      & 73.07              & 0\%                        & 0\%                        & 31.07\%                    & 56.11\%                    & 12.82\%            \\
      Graycode        & 18.36              & 0\%                        & 0\%                        & 27.24\%                    & 54.48\%                    & 18.28\%            \\
      LFSR            & 35.97              & 0\%                        & 0\%                        & 23.91\%                    & 49.20\%                    & 26.89\%            \\
      LZC             & 38.36              & 0\%                        & 0\%                        & 40.15\%                    & 52.14\%                    & 7.72\%             \\
      RR Arbiter      & 184.52             & 0\%                        & 0\%                        & 34.85\%                    & 60.37\%                    & 4.78\%             \\
      Snitch          & 59.12              & 0\%                        & 0\%                        & 27.91\%                    & 64.27\%                    & 7.82\%             \\
      Stream Delayer  & 11.43              & 0\%                        & 0\%                        & 21.00\%                    & 54.25\%                    & 24.75\%            \\
      \bottomrule
    \end{tabular}
  }
  \caption[Detailed performance evaluation of the optimized MLIR-based simulator.]{Detailed performance evaluation of the optimized MLIR-based simulator. For the specification of the reported values please refer to the description of Table \ref{tab:time_detail0}.}
  \label{tab:time_detail1}
\end{table}

We see a significant performance improvement with the changes we applied. Also, given the faster simulation speeds we can obtain, we now observe our timing method introduces significant overhead, as high as $37\%$ in the worst case, due to the millions of calls into the \textit{C++} \texttt{std::chrono} library. These overheads where of negligible impact on our first, slower implementation. Tracking the simulation timing at this granularity is an optional feature, and not of general interest in a simulation. We thus provide a further timing report in Table \ref{tab:timings_compare1}, where we do not use any additional flags other than the required \textit{root} entity specification, and disabling the trace generation. Here we also report the timings of the reference simulators as in Table \ref{tab:timings_compare}, to make it easier for the reader to compare the new timings against the reference simulators.

\begin{table}[ht]
  \centering
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccc}
      \toprule
      \textbf{Design} & \textbf{MLIR Simulator [s]} & \begin{tabular}[c]{@{}l@{}}\textbf{LLHD Reference }\\\textbf{Simulator [s]}\end{tabular} & \textbf{LLHD Blaze}~ \textbf{[s]} & \textbf{Vivado [s]} \\
      \toprule
      CDC (Gray)      & 29.16                       & 4894.94                    & 16.56                             & 12.00               \\
      CDC (strobe)    & 36.57                       & 5132.95                    & 16.13                             & 16.10               \\
      FIFO Queue      & 10.88                       & 2324.60                    & 11.77                             & 5.30                \\
      FIR Filter      & 63.75                       & 14807.50                   & 18.00                             & 33.10               \\
      Graycode        & 15.72                       & 14628.93                   & 9.79                              & 23.00               \\
      LFSR            & 26.32                       & 7080.40                    & 25.20                             & 37.20               \\
      LZC             & 36.84                       & 31207.36                   & 5.87                              & 8.00                \\
      RR Arbiter      & 177.08                      & 84072.45                   & 17.55                             & 75.50               \\
      Snitch          & 56.40                       & 6721.92                    & 48.55                             & 4.80                \\
      Stream Delayer  & 10.41                       & 1136.31                    & 8.74                              & 2.85                \\
      \bottomrule
    \end{tabular}
  }
  \caption[Performance comparison with the optimized MLIR-based simulator.]{Performance comparison with the optimized MLIR-based simulator. The values for the reference simulators are reported as in Figure \ref{tab:timings_compare}.}
  \label{tab:timings_compare1}
\end{table}

We observe that we can now surpass the performance of Vivado's simulator in two cases and LLHD-Blaze's performance in one case. We though notice that our simulator still does not scale well on more complex examples, \eg, our simulator runs $11.8$ times slower than Vivado for the Snitch example, compared to the \textit{Graycode} example, which runs $1.5$ times faster.

Profiling of the execution shows a great majority of the time is still used creating new events in the queue ($\sim$$50\%$) and processing the events ($\sim$$25\%$), while the execution of JIT-compiled instances' themselves still takes a relatively small amount of the total execution ($\sim$$10\%$). We are confident that further optimization of the current implementation could bring this sequential approach to truly competitive performance.




% In Table \ref{tab:time_detail1} we list the timing results we obtain after just applying some more consideration in the used data structures, such as using LLVM \texttt{SmallVector}s instead of standard vectors where fit, as well as avoiding looking up the converted units' function pointers through the JIT, and instead storing them into the state.

% \begin{table}
%   \centering
%   \resizebox{\linewidth}{!}{%
%     \begin{tabular}{lllllll}
%       \toprule
%       \textbf{Design} & \textbf{Total [s]} & \begin{tabular}[c]{@{}l@{}}\textbf{Init}\\\textbf{phase [s]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Allocation}\\\textbf{phase [s]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Signals}\\\textbf{update [s]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Instances}\\\textbf{execution [s]}\end{tabular} & \textbf{Remainder [s]} \\
%       \toprule
%       CDC (Gray)      & 421.95             & 0                          & 0                          & 54.50                      & 301.60                     & 65.85                  \\
%       CDC (strobe)    & 622.60             & 0                          & 0                          & 64.56                      & 468.56                     & 89.49                  \\
%       FIFO Queue      & 140.73             & 0                          & 0                          & 17.20                      & 101.90                     & 21.63                  \\
%       FIR Filter      & 745.64             & 0                          & 0                          & 75.20                      & 581.30                     & 89.14                  \\
%       Graycode        & 223.71             & 0                          & 0                          & 17.50                      & 184.00                     & 22.21                  \\
%       LFSR            & 636.52             & 0                          & 0                          & 63.90                      & 508.30                     & 64.32                  \\
%       LZC             & 156.65             & 0                          & 0                          & 17.10                      & 120.40                     & 19.15                  \\
%       RR Arbiter      & 1065.21            & 0                          & 0                          & 140.20                     & 715.90                     & 209.11                 \\
%       Snitch          & 384.24             & 0                          & 1                          & 44.90                      & 258.60                     & 79.74                  \\
%       Stream Delayer  & 139.72             & 0                          & 0                          & 14.50                      & 102.10                     & 23.12                  \\
%       \bottomrule
%     \end{tabular}
%   }
%   \caption{Detailed break down of the simulation timings, after a couple of small adjustments.}
%   \label{tab:time_detail1}
% \end{table}

% With just those smaller considerations, we already observe a performance improvement by an average factor of $4$, showing that our current implementation is far from being optimal.

% Inspecting the results from a performance monitoring tool, such as \texttt{perf}, we observe that the majority of the time in our simulator is spent in re-ordering and popping the queue, indicating our event queue structure is not yet optimal. This also affects the time required to execute the units, since driving a signal implies interacting with the event-queue.

% We also observe that setting the LLVM code-gen optimization level to $O3$ does not translate to any changes in most of the examples from the LLVM IR code generated with the $O2$ flag. We could also observe the same behavior with the LLHD Blaze simulator. We deduce that the generated code after the optimizations enabled by the $O2$ flag mostly contains memory interaction (either with the state or with signals) and runtime function calls (signal drives) that can generally not be further optimized. Improving on those aspects could further improve the amount of optimization that could be applied to the LLVM IR code we are generating.