%04-evaluation.tex
\chapter{Evaluation}
\label{ch:eval}

In this chapter, we evaluate the correctness and the performance of our MLIR-based simulator using the same test set used in the LLHD paper~\cite{Schuiki2020}, including a full \textit{Snitch} processor core~\cite{Zaruba2020}. We use \texttt{moore} version $0.6.0$ as the reference compiler to get the SystemVerilog test set in LLHD (with no additional flags active during compilation), paired with Martin Erhart's MLIR writer, to convert the original LLHD syntax to the MLIR syntax. The only manual modification applied to the compiled designs is removing some drives, erroneously inserted by \texttt{moore}, and that would otherwise break the simulation by resetting the affected signal to its default value (\eg, $0$ for integers) after a $0$-time delay. This occurred in the \textit{CDC (Gray)}, \textit{Graycode}, \textit{RR Arbiter}, and \textit{Snitch} examples.

We note that the version of \texttt{moore} we're using does not correspond to the latest available version, but it is the most tested, and we can guarantee that the LLHD output it generates is well-formed. When using the latest version ($0.10.0$ at the time of writing), we noticed some discrepancies that could be either due to edge-cases not covered in our simulator or bugs introduced in \texttt{moore} after some major refactorings. We note that in \texttt{moore} version $0.11.0$, released after we generated our evaluation, those discrepancies seem to have been lifted, though a more accurate inspection would be required. All the examples of the test set, as well as the scripts used to generate the evaluations, can also be found on our public testing GitHub repo\footnote{\url{https://github.com/rodonisi/llhd-extras}}.

%---------------------------------------------------------------------------------------------------

\section{Simulator Correctness}
\label{sec:correctness}
We test for correctness by comparing the trace generated by our simulator for the top-level signals (using the variant 4. of the trace format, described in Section~\ref{sec:trace}) against the one generated by Xilinx's Vivado's simulator~\cite{vivado}, once again only keeping the top-level signals of the design (\ie, the test-bench signals).

To obtain a reliable comparison, we wrote a simple python script to translate the VCD trace generated by Vivado to our textual format (using the \textit{vcdvcd}~\cite{vcdvcd} python module to parse the input VCD file). We then proceed by generating a diff (using the \texttt{diff} command-line tool) of the full trace generated by Vivado, against the full trace we obtain with our simulator.

We remark that since we cannot currently save the port names for the various instance arguments, a comparison of the traces below the top-level becomes a non-trivial task. For the top-level though, where all the signal names are dictated by the original signal names, rather than the port names, we can trivially perform a comparison. Furthermore, by treating the underlying circuit as a “black box“ and observing its output signals, we can still infer the correctness of our simulation, as those signals depend on the correct behavior of the circuit.

We observe that by using LLHD, we are simulating slightly different designs from the original SystemVerilog implementations, due to the lack of a logical type in the current state of LLHD, which is vastly used in SystemVerilog designs. We thus expect slight variations in the simulation traces, but only in cases where a logic type takes either a “don't care“ value (\textit{X}) or a high-impedance value (\textit{Z}). An example can be seen in Listing~\ref{listing:diff-lfsr}.

\begin{listing}
  \lstset{
    escapeinside=||,
    basicstyle=\footnotesize\ttfamily,
    stepnumber=1
  }
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  |\textcolor{red}{lfsr\_16bit\_tb/clk\_i  x}|
  |\textcolor{red}{lfsr\_16bit\_tb/en\_i  x}|
  |\textcolor{red}{lfsr\_16bit\_tb/out\_o  x}|
  lfsr_16bit_tb/rst_ni  0x01
1000ps
  lfsr_16bit_tb/out_o  0x0001
  lfsr_16bit_tb/rst_ni  0x00
2000ps
  lfsr_16bit_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  |\textcolor{ForestGreen}{lfsr\_16bit\_tb/clk\_i  0x00}|
  |\textcolor{ForestGreen}{lfsr\_16bit\_tb/en\_i  0x00}|
  |\textcolor{ForestGreen}{lfsr\_16bit\_tb/out\_o  0x0000}|
  lfsr_16bit_tb/rst_ni  0x01
1000ps
  lfsr_16bit_tb/out_o  0x0001
  lfsr_16bit_tb/rst_ni  0x00
2000ps
  lfsr_16bit_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \caption[Side-by-side zoom-in of the LFSR example's diff.]{Side-by-side zoom-in of the LFSR example's diff. Here we can see the case where a \texttt{X} value in Vivado's trace (left) is not reflected in our trace (right), as it cannot currently be represented in LLHD. Note that this excerpt contains all the differences for this example.}
  \label{listing:diff-lfsr}
\end{listing}

A further case where we expect differences is where we do not observe a change due to a previous \texttt{X} or \texttt{Z} value. Listing~\ref{listing:diff-lzc} shows an example where the change at $1000ps$ for signal \textit{cnt\_o} is not reflected in our trace, as the signal is already on that value (at line $3$ of our trace).


\begin{listing}
  \lstset{
    escapeinside=||,
    basicstyle=\footnotesize\ttfamily,
    stepnumber=1
  }
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  |\textcolor{red}{lzc\_tb/clk\_i  x}|
  |\textcolor{red}{lzc\_tb/cnt\_o  x}|
  |\textcolor{red}{lzc\_tb/empty\_o  x}|
  |\textcolor{red}{lzc\_tb/in\_i  x}|
  lzc_tb/rst_ni  0x01
1000ps
  |\textcolor{red}{lzc\_tb/cnt\_o  0x0f}|
  lzc_tb/empty_o  0x00
  lzc_tb/in_i  0x0001
  lzc_tb/rst_ni  0x00
2000ps
  lzc_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  |\textcolor{ForestGreen}{lzc\_tb/clk\_i  0x00}|
  |\textcolor{ForestGreen}{lzc\_tb/cnt\_o  0x0f}|
  |\textcolor{ForestGreen}{lzc\_tb/empty\_o  0x01}|
  |\textcolor{ForestGreen}{lzc\_tb/in\_i  0x0000}|
  lzc_tb/rst_ni  0x01
1000ps

  lzc_tb/empty_o  0x00
  lzc_tb/in_i  0x0001
  lzc_tb/rst_ni  0x00
2000ps
  lzc_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \caption[Side-by-side zoom-in of the LZC example's diff.]{Side-by-side zoom-in of the LZC example's diff. Here we can see the case where an update from an \texttt{X} value to some other value is not reflected in our trace, if the signal is already carrying the new value in the LLHD model. Note that this excerpt contains all the differences for this example.}
  \label{listing:diff-lzc}
\end{listing}

The only exception not falling under those cases is in the \textit{Stream Delayer} example, where we can see a mismatch in the value initialization not caused by \texttt{X} values, as shown in Listing~\ref{listing:diff-sd}. By inspecting our full trace, we observe that we initialize those signals to the correct values, but they are driven at a time sub-step of the initial real-time step ($0$ps). We presume that Vivado is merging the initial time sub-steps into the next step, rather than the $0$ps step, as we do. We believe our approach to be correct, since treating the initial step as a special case makes the simulation trace less consistent. An additional toggle to enable this behavior, though, could be added to minimize the discrepancies between the simulation traces if what we believe to be Vivado's approach results in being a common practice in well-established simulators.

\begin{listing}
  \lstset{
    escapeinside=||,
    basicstyle=\footnotesize\ttfamily,
    stepnumber=1
  }
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  stream_delay_tb/clk_i  x
  [...]
  |\textcolor{red}{stream\_delay\_tb/ready\_i  0x00}|
  |\textcolor{red}{stream\_delay\_tb/ready\_o  0x00}|
  stream_delay_tb/rst_ni  0x01
  stream_delay_tb/valid_i  0x01
  |\textcolor{red}{stream\_delay\_tb/valid\_o  0x00}|
1000ps
  |\textcolor{red}{stream\_delay\_tb/ready\_i  0x01}|
  |\textcolor{red}{stream\_delay\_tb/ready\_o  0x01}|
  stream_delay_tb/rst_ni  0x00
  |\textcolor{red}{stream\_delay\_tb/valid\_o  0x01}|
2000ps
  stream_delay_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \begin{lstlisting}
0ps
  stream_delay_tb/clk_i  0x00
  [...]
  |\textcolor{ForestGreen}{stream\_delay\_tb/ready\_i  0x01}|
  |\textcolor{ForestGreen}{stream\_delay\_tb/ready\_o  0x01}|
  stream_delay_tb/rst_ni  0x01
  stream_delay_tb/valid_i  0x01
  |\textcolor{ForestGreen}{stream\_delay\_tb/valid\_o  0x01}|
1000ps


  stream_delay_tb/rst_ni  0x00

2000ps
  stream_delay_tb/rst_ni  0x01
...
        \end{lstlisting}
  \end{minipage}
  \caption[Side-by-side zoom-in of the Stream Delayer example's diff.]{Side-by-side zoom-in of the Stream Delayer example's diff. Here we can see the case where we have an initialization mismatch. Some signal changes have been omitted and replaced by a [...] designation for the sake of readability. Note that this excerpt contains all the differences for this example.}
  \label{listing:diff-sd}
\end{listing}

Considering those cases as exceptions, we can assert that we are generating equivalent (though not equal) simulation traces to Vivado's simulator for our test set. Most notably, for the \textit{Graycode} example, we can generate a $100\%$ matching trace. The \textit{Snitch} processor on the other side is the most complex example, and the one with most differences, but all of them fall under the cases described above. A full listing of the raw \texttt{diff}-generated differentials for all traces can be found in Appendix~\ref{app:diffs}.

%---------------------------------------------------------------------------------------------------

\section{Performance Evaluation}
We evaluate the performance of our simulator by taking the median value over $10$ different runs for each design, as an indication of its average runtime that is not too sensitive to outliers. Also, when we mention average slow-downs/speed-ups, we use the geometric mean as the metric.

We disable the trace generation during performance evaluation. The rationale is that we want to measure the performance of the actual simulation, without the bias introduced by trace processing and possible I/O bounds. For example, the processing required to generate the trace format used in Section~\ref{sec:correctness} sometimes requires alone more than $3$ times the time of the actual simulation in the current implementation. We also consider that a user might not require generating a signal trace dump but might rather be more interested in testing whether a test-bench can simulate without errors or triggering assertions, though we note that such features are not yet implemented in LLHD.

All the reported run-times have been generated by running our simulator in release mode on a Linux machine equipped with an i5 $750$ processor running at $2.67Ghz$, and $8GB$ of RAM. Each series of reports is also generated after a fresh restart.

\begin{table}[ht]
  \centering
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccc}
      \toprule
      \textbf{Design} & \begin{tabular}[c]{@{}l@{}}\textbf{MLIR-based}\\\textbf{Simulator [s]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{LLHD Reference }\\\textbf{Simulator [s]*}\end{tabular} & \textbf{LLHD Blaze}~ \textbf{[s]} & \textbf{Vivado [s]} \\
      \toprule
      CDC (Gray)      & 421.80                    & 4894.94                   & 16.55                             & 12.00               \\
      CDC (strobe)    & 622.75                    & 5132.95                   & 16.10                             & 16.00               \\
      FIFO Queue      & 140.22                    & 2324.60                   & 11.77                             & 5.00                \\
      FIR Filter      & 746.35                    & 14807.50                  & 17.97                             & 33.00               \\
      Graycode        & 222.60                    & 14628.93                  & 9.78                              & 23.00               \\
      LFSR            & 636.52                    & 7080.40                   & 25.20                             & 37.00               \\
      LZC             & 155.63                    & 31207.36                  & 5.86                              & 8.00                \\
      RR Arbiter      & 1062.82                   & 84072.45                  & 17.55                             & 75.50               \\
      Snitch          & 382.53                    & 6721.92                   & 48.53                             & 4.81 **             \\
      Stream Delayer  & 139.44                    & 1136.31                   & 8.72                              & 2.84 **             \\
      \bottomrule
    \end{tabular}
  }

  \caption[Comparison of simulation timings for our first simulator prototype.]{Comparison of the time required to simulate the design of our test set on various simulators. All time values are reported in seconds and obtained by summing the \textit{user} and \textit{system} values reported by the \textit{time} command-line utility, or, in Vivado's case, the \textit{elapsed} value reported on simulation completition. \\ * Due to the amount of time required by the LLHD reference simulator, the timings are reported for only one run. \\ ** Due to Vivado not reporting the simulation time after the completition for this case, we fell back to using the “\mintinline{Shell}{time {run all}}“ command. This command otherwise made Vivado often crash for longer simulations, which is why we only use it as a fall-back.}
  \label{tab:timings_compare}
\end{table}

Table~\ref{tab:timings_compare} lists the median timings we obtain with our first prototype of the simulator that can run all the examples from our test set. As a reference, we compare the performance we obtain against the LLHD reference simulator\footnote{Run with no additional flags.}, the LLHD Blaze simulator\footnote{Run with no additional flags.}, and the Vivado simulator\footnote{Always running a fresh simulation through the \mintinline{Shell}{run all} TCL command from the Vivado IDE.}. A full and raw listing of the timing results used to generate this evaluation can be found in Appendix~\ref{app:timings}.

We observe that our simulator is nowhere near obtaining competitive performance when compared against both a commercial simulator and LLHD Blaze (which implements a very similar approach to ours). In particular, our performance is on average $26$ times slower than Vivado's simulator.
In table \ref{tab:time_detail0} we explore a more detailed view of the time spent in various stages of the simulation. Once again the listed values are averaged over multiple executions of the simulator, and full listings for each example can be found in Appendix \ref{app:timings}.

\begin{table}[ht]
  \centering
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccccc}
      \toprule
      \textbf{Design} & \textbf{Total [s]} & \begin{tabular}[c]{@{}l@{}}\textbf{Init}\\\textbf{phase}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Allocation}\\\textbf{phase}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Signals}\\\textbf{update}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Instances}\\\textbf{execution}\end{tabular} & \textbf{Remainder} \\
      \toprule
      CDC (Gray)      & 421.80             & 0\%                       & 0\%                        & 12.80\%                    & 71.48\%                    & 15.72\%            \\
      CDC (strobe)    & 622.75             & 0\%                       & 0\%                        & 10.36\%                    & 75.15\%                    & 14.49\%            \\
      FIFO Queue      & 140.22             & 0\%                       & 0\%                        & 12.12\%                    & 72.39\%                    & 15.49\%            \\
      FIR Filter      & 746.35             & 0\%                       & 0\%                        & 10.05\%                    & 77.98\%                    & 11.97\%            \\
      Graycode        & 222.60             & 0\%                       & 0\%                        & 7.64\%                     & 81.99\%                    & 10.38\%            \\
      LFSR            & 636.52             & 0\%                       & 0\%                        & 10.05\%                    & 79.81\%                    & 10.14\%            \\
      LZC             & 155.63             & 0\%                       & 0\%                        & 10.92\%                    & 76.78\%                    & 12.29\%            \\
      RR Arbiter      & 1062.82            & 0\%                       & 0\%                        & 13.08\%                    & 67.18\%                    & 19.74\%            \\
      Snitch          & 382.53             & 0\%                       & 0\%                        & 11.76\%                    & 67.05\%                    & 20.92\%            \\
      Stream Delayer  & 139.44             & 0\%                       & 0\%                        & 10.04\%                    & 73.15\%                    & 16.81\%            \\
      \bottomrule
    \end{tabular}
  }
  \caption[Detailed performance evaluation of the MLIR-based simulator]{Detailed performance evaluation of the MLIR-based simulator. The \textit{Total} column indicates the total time required to simulate the design from start to end, as reported by the \texttt{time} command-line tool. The remaining columns report the overall percentage of the time spent in that simulation phase. The \textit{Init phase} column includes initializing the state structures, converting the LLHD module to the LLVM dialect, and initializing the JIT compiler. The \textit{Allocation phase} includes executing the generated initialization function. The \textit{Signals update} and \textit{Instances execturion} columns include processing and applying the scheduled updates, and execution the various instances respectively. The \textit{Remainder} metric reports the percentage of time spent in stages we do not track, including constructing and destroying the simulator, iterating over, and popping the event queue.}
  \label{tab:time_detail0}
\end{table}

We recognize our first prototype prioritized correctness over performance, and as such many of the used data structures and approaches are far from being optimal. Though, we can see an interesting result in the initialization times, where we see the negligible impact of MLIR applying the LLHD-to-LLVM conversion pass (part of the \textit{init phase} metric), as well as the execution of the generated initialization function (\textit{allocation phase} metric). In both cases, the total time required rounds down to $0\%$ of the total simulation time during the formatting of the results. We also see that a great majority of the required simulation time resides in the execution of the various instances. Due to time constraints, we were not able to bring major changes targeted at improving performance, but we observe that some performance optimizations include some low-hanging fruits.

By inspecting the simulator execution with a profiling tool, \texttt{perf}, we get a clearer picture of where we are losing performance. We see that most of the time is spent allocating and deallocating memory for the simulator queue and state-keeping structures. Also, we note that the execution of our lowered LLHD code takes a negligible amount of time (generally around $1$-$2\%$).

Given the profiling results, we applied the following changes, achieving the timing results listed in Table \ref{tab:time_detail1}:

\begin{enumerate}
  \item An overall cleanup of the code and replacement of basic data structures with more efficient implementations, \eg, using LLVM \texttt{SmallVector} instead of standard vectors (where fit).
  \item Initially, we were executing the units through the JIT engine's \texttt{invoke} function. Looking up the compiled function pointers through the JIT compiler turns out to be very expensive, so we switched to storing the compiled function pointers in the instance structures, making them readily available when needed.
  \item We used to map instances to the respective structures by name, requiring us to perform lots of string allocations and comparisons when iterating over the wakeup queue. We thus switched to an integer indexing, more similar to what we were already doing with signals, dramatically cutting down on strings usage.
  \item We observe that the number of events present at the same time in the event queue is fairly small, usually between $2$ and $6$. Given this, we can change the event queue to be a simple, unsorted vector, where we search for the earliest slot in each cycle instead of sorting the queue for each drive. Furthermore, this allows us to reuse the already allocated slots instead of popping and destroying them, significantly cutting down the allocation/deallocation times. We applied the same approach to the slot change buffers, replacing the signal-to-changes map structure with a vector of index-change pairs, and reusing the buffers instead of allocating new ones each time. This also required slightly reworking the signal processing phase, to ensure no additional trace entries and instance wakeups get triggered.
  \item Initially, we used to search through the whole event queue for an existing slot with the given timestamp, for each drive. We avoid redundant iteration over the queue when performing a drive, by:
        \begin{enumerate}
          \item Keeping track of the index of the current earliest event (the \textit{top} of the queue), such that multiple drives to the “next event“ can be directly added to the correct slot without any searching. This could happen, for example, when all bits of a signal are driven independently with the same delay.
          \item If a drive has a timestamp earlier than the current top of the queue, it cannot be part of an existing slot, otherwise it would be the top of the queue. In this case, we directly create a new slot and update the current top index.
          \item If a drive has a timestamp later than the current top, a slot for that event could already be present in the queue. In this case, we need to search the queue for an existing slot and create a new slot if none is found.
        \end{enumerate}
\end{enumerate}

\begin{table}[ht]
  \centering
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccccc}
      \toprule
      \textbf{Design} & \textbf{Total [s]} & \begin{tabular}[c]{@{}l@{}}\textbf{Init}\\\textbf{phase}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Allocation}\\\textbf{phase}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Signals}\\\textbf{update}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Instances}\\\textbf{execution}\end{tabular} & \textbf{Remainder} \\
      \toprule
      CDC (Gray)      & 36.15              & 0\%                        & 0\%                        & 27.66\%                    & 47.02\%                    & 25.32\%            \\
      CDC (strobe)    & 44.65              & 0\%                        & 0\%                        & 29.11\%                    & 51.51\%                    & 19.37\%            \\
      FIFO Queue      & 11.65              & 0\%                        & 0\%                        & 25.76\%                    & 42.93\%                    & 31.31\%            \\
      FIR Filter      & 72.96              & 0\%                        & 0\%                        & 23.30\%                    & 53.46\%                    & 23.24\%            \\
      Graycode        & 19.24              & 0\%                        & 0\%                        & 20.79\%                    & 57.18\%                    & 22.02\%            \\
      LFSR            & 36.87              & 0\%                        & 0\%                        & 21.70\%                    & 46.11\%                    & 32.19\%            \\
      LZC             & 33.04              & 0\%                        & 0\%                        & 18.16\%                    & 54.48\%                    & 27.36\%            \\
      RR Arbiter      & 199.22             & 0\%                        & 0\%                        & 23.84\%                    & 51.70\%                    & 24.46\%            \\
      Snitch          & 66.11              & 0\%                        & 0\%                        & 21.18\%                    & 46.89\%                    & 31.93\%            \\
      Stream Delayer  & 11.59              & 0\%                        & 0\%                        & 25.89\%                    & 43.15\%                    & 30.96\%            \\
      \bottomrule
    \end{tabular}
  }
  \caption[Detailed performance evaluation of the optimized MLIR-based simulator.]{Detailed performance evaluation of the optimized MLIR-based simulator. For the specification of the reported values please refer to the description of Table \ref{tab:time_detail0}.}
  \label{tab:time_detail1}
\end{table}

We see a significant performance improvement with the changes we applied. Also, given the faster simulation speeds we can obtain, we now observe our timing method introduces significant overhead, as high as $40\%$ in the worst case, due to the millions of calls into the \textit{C++} \texttt{std::chrono} library. These overheads where of negligible impact on our first, slower implementation. Tracking the simulation timing at this granularity is an optional feature, mostly targeted at debugging the simulator and not of general interest in a simulation. We thus provide a final timing report in Table~\ref{tab:timings_compare1}, where we do not use any additional flags other than the required \textit{root} entity specification, and disabling the trace generation. Here we also report the timings of the reference simulators as in Table \ref{tab:timings_compare}, to make it easier for the reader to compare the new timings against the reference simulators.

\begin{table}[ht]
  \centering
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccc}
      \toprule
      \textbf{Design} & \begin{tabular}[c]{@{}l@{}}\textbf{MLIR-based}\\\textbf{Simulator [s]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{LLHD Reference }\\\textbf{Simulator [s]}\end{tabular} & \textbf{LLHD Blaze}~ \textbf{[s]} & \textbf{Vivado [s]} \\
      \toprule
      CDC (Gray)      & 32.84                      & 4894.94                    & 16.55                             & 12.00               \\
      CDC (strobe)    & 39.52                      & 5132.95                    & 16.10                             & 16.00               \\
      FIFO Queue      & 10.19                      & 2324.60                    & 11.77                             & 5.00                \\
      FIR Filter      & 63.76                      & 14807.50                   & 17.97                             & 33.00               \\
      Graycode        & 16.53                      & 14628.93                   & 9.78                              & 23.00               \\
      LFSR            & 26.34                      & 7080.40                    & 25.20                             & 37.00               \\
      LZC             & 31.25                      & 31207.36                   & 5.86                              & 8.00                \\
      RR Arbiter      & 190.17                     & 84072.45                   & 17.55                             & 75.50               \\
      Snitch          & 64.10                      & 6721.92                    & 48.53                             & 4.81                \\
      Stream Delayer  & 10.52                      & 1136.31                    & 8.72                              & 2.84                \\
      \bottomrule
    \end{tabular}
  }
  \caption[Performance comparison with the optimized MLIR-based simulator.]{Performance comparison with the optimized MLIR-based simulator. The values for the reference simulators are reported as in Figure \ref{tab:timings_compare}.}
  \label{tab:timings_compare1}
\end{table}

We observe that we can now surpass the performance of Vivado's simulator in two cases and LLHD-Blaze's performance in one case. We though notice that our simulator still does not scale as well as Vivado on more complex examples, \eg, our simulator runs $13.3$ times slower than Vivado for the \textit{Snitch} example, compared to the \textit{Graycode} and \textit{LFSR} examples, which both run $1.4$ times faster.

Profiling of the execution shows a great majority of the time is still used creating new events and processing them, while the execution of JIT-compiled instances' themselves still takes a relatively small amount of the total execution time. \Eg, for the \textit{RR Arbiter} design, our longest-running example, they take $\sim$$43\%$, $\sim$$24\%$, and $\sim$$8\%$ of the total simulation time respectively. After the changes in point 4., we introduced sorting of the slot changes to enable processing one signal at a time, and ensure no extra trace entries and instance wakeups were erroneously triggered. This new step, part of the \textit{remainder} metric, also takes a considerable amount of time, at $\sim$$20\%$. We are confident that further optimization of the current implementation could bring this sequential approach to truly competitive performance.

