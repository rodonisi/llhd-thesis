%04-evaluation.tex
\chapter{Evaluation}
\label{ch:eval}

In this chapter, we evaluate the correctness and the performance of our MLIR-based simulator using the same test set used in the LLHD paper \cite{Schuiki2020}, including a full \textit{Snitch} processor \cite{Zaruba2020}. We use \texttt{moore} version $0.6.0$ as the reference compiler to get the SystemVerilog test set in LLHD, paired with Martin Erhart's MLIR writer to convert the original LLHD syntax to the MLIR syntax. We note that the version of \texttt{moore} we're using doesn't correspond to the latest available version, but it is the most tested, and we can guarantee that the LLHD output it generates is well-formed. When using the latest version ($0.10.0$ at the time of writing), we noticed some discrepancies that could be either due to edge-cases not covered in our simulator, or bugs introduced in \texttt{moore} after some major refactorings.

%---------------------------------------------------------------------------------------------------

\section{Simulator Correctness}
\label{sec:correctness}
We test for correctness by comparing the trace generated by our simulator for the top-level signals (with the LLHD helper signals filtered out, variant 4. described in Section \ref{sec:trace}) with the one generated by simulating with Xilinx's Vivado, once again only keeping the signals of the top-level of the design (\ie the test-bench).

To obtain a reliable comparison, we wrote a simple python script to translate the VCD trace generated by Vivado to our textual format. We then proceed by generating a diff (using the \texttt{diff} command-line tool) of the full trace generated by Vivado \cite{vivado}, against the trace we obtain with our simulator.
%first $1000$ns of simulation trace. 
%The rationale for checking only a first part of the trace, is that the signal waveforms generated by a test-bench usually follow a very regular pattern. This means that significant changes in traces should reveal themselves early rather than later in the simulation trace. 
A full listing of the differential of all traces can be found in Appendix \ref{app:diffs}. We remark that since we cannot currently save the port names for the various instance's signals, a comparison of the traces below the top-level becomes a non-trivial task. For the top-level though, where all the signal names are not dictated by port names, but by the original signal names, we can trivially perform a comparison. Furthermore, by treating the underlying circuit as a “black box“ and observing its output signals, we can still infer the correctness of our simulation, as those signals depend on the correct behavior of the circuit.

We observe that by using LLHD, we are simulating slightly different designs from the original SystemVerilog implementations, due to the current lack of a logical type in our LLHD implementation, which is vastly used in SystemVerilog designs. We thus expect slight variations in the simulation traces, but only in cases where a logic type takes either a “don't care“ value (\textit{X}) or a high-impedance value (\textit{Z}), or consequences of those case. More precisely, the most common case is where we can observe an \textit{X} value in Vivado's trace, and observe a value in the LLHD trace. The signal value later converges for the two simulators, with an additional entry in Vivado's trace, not present in ours, creating a further difference. We can see an example of this in the LFSR's trace comparison from Listing \ref{listing:diff-lfsr}

\begin{listing}
    \inputminted[frame=lines,
        framesep=2mm,
        baselinestretch=1.0,
        % bgcolor=lightgray!10,
        fontsize=\footnotesize, linenos]{c}{diffs/diff-lfsr.txt}
    \caption{The full diff generated by comparing the traces generated by our simulator and Vivado's simulator.}
    \label{listing:diff-lfsr}
\end{listing}

Most notably, for the \textit{Graycode} example, we can generate a $100\%$ matching trace. The \textit{Snitch} processor on the other side is the most complex example, and the one with most differences, but all of them are due to the reasons explained above.

%---------------------------------------------------------------------------------------------------

\section{Performance Evaluation}
We evaluate performance of our simulator by averaging the timing results over $10$ different runs for each design, in order to avoid major variations. We also disable the trace generation during performance evaluation. The rationale here is that we want to measure the performance of the actual simulation, without the bias introduced by trace processing and possible I/O bounds. For example, the processing required in the current implementation to generate the trace format used in Section \ref{sec:correctness} sometimes requires alone more than $3$ times the time of the actual simulation. We also consider that a user might not require generation signal trace dump, but might rather be more interested in testing whether a test-bench can simulate without errors or triggering assertions, though we note that such features are not yet implemented in LLHD.

All the reported run-times have been generated by running our simulator in release mode on a Linux machine equipped with an i5 $750$ processor running at $2.67Ghz$, and $8GB$ of RAM. Each series of report is also generated after a fresh restart.

\begin{table}
    \centering
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{lllll}
            \toprule
            \textbf{Design} & \begin{tabular}[c]{@{}l@{}}\textbf{MLIR }\\\textbf{Simulator [s]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{LLHD Reference }\\\textbf{Simulator [s] **}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{LLHD }\\\textbf{Blaze [s]}\end{tabular} & \textbf{Vivado [s]} \\
            \toprule
            CDC (Gray)      & 428.50                    & 4894.94                   & 16.56                     & 12.00               \\
            CDC (strobe)    & 648.41                    & 5132.95                   & 16.13                     & 16.10               \\
            FIFO Queue      & 144.50                    & 2324.60                   & 11.77                     & 5.30                \\
            FIR Filter      & 753.92                    & 14807.50                  & 18.00                     & 33.10               \\
            Graycode        & 231.11                    & 14628.93                  & 9.79                      & 23.00               \\
            LFSR            & 666.31                    & 7080.40                   & 25.20                     & 37.20               \\
            LZC             & 150.57                    & 31207.36                  & 5.87                      & 8.00                \\
            RR Arbiter      & 1055.44                   & 84072.45                  & 17.55                     & 75.50               \\
            Snitch          & 394.57                    & 0.00                      & 48.55                     & 16.60               \\
            Stream Delayer  & 147.91                    & 0.00                      & 8.74                      & 9.00                \\
            \bottomrule
        \end{tabular}
    }
    \caption[Comparison of simulation timings.]{Comparison of the time required to simulate the design of our test set on various simulators. The \texttt{MLIR Simulator} reports the timings obtained by the first implementation of our MLIR-based simulator. All time values are reported in seconds. \\ ** Due to the amount of time required by the LLHD reference simulator, the timings are reported for only one run.}
    \label{tab:timings_compare}
\end{table}

Table \ref{tab:timings_compare} lists the average timings we obtain with the first prototype of the simulator that is able to run all the examples from our test set. As reference, we compare the performance we obtain against the LLHD reference simulator, the LLHD blaze simulator and the Vivado simulator. A full listing of the timings results can be found in Appendix \ref{app:timings}.


We observe that our simulator is nowhere near obtaining competitive performance when compared against both a commercial simulator, and LLHD blaze, which implements a very similar approach to ours. In particular, our performance is in average $23$ times slower than Vivado's simulator.

In table \ref{tab:time_detail0} we explore a more detailed view of the time spent in various stages of the simulation. Once again the listed values are averaged over multiple executions of the simulator, and full listings for each example can be found in Appendix \ref{app:timings}.

\begin{table}
    \centering
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{llllll}
            \toprule
            \textbf{Design} & \textbf{Total {[}s{]}} & \textbf{\begin{tabular}[c]{@{}l@{}}Signals\\ update {[}s{]}\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Gather instance\\ arguments {[}s{]}\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}JIT execution\\ time {[}s{]}\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Remainder\\ {[}s{]}\end{tabular}} \\ \toprule
            Graycode        & 231.11                 & 18.00                              & 12.00                              & 172.60                             & 28.51                              \\
            LFSR            & 666.31                 & 73.60                              & 43.80                              & 463.50                             & 85.41                              \\
            CDC (strobe)    & 648.41                 & 63.50                              & 31.10                              & 448.00                             & 105.81                             \\
            Stream Delayer  & 147.91                 & 16.00                              & 8.20                               & 95.40                              & 28.31                              \\
            FIFO Queue      & 144.50                 & 16.90                              & 5.80                               & 96.70                              & 25.10                              \\
            LZC             & 150.57                 & 17.00                              & 5.00                               & 107.60                             & 20.97                              \\
            RR Arbiter      & 1055.44                & 137.10                             & 25.30                              & 670.60                             & 222.44                             \\
            FIR Filter      & 753.92                 & 75.00                              & 29.40                              & 546.10                             & 103.42                             \\
            CDC (Gray)      & 428.50                 & 52.50                              & 19.70                              & 280.20                             & 76.10                              \\
            Snitch          & 394.57                 & 45.50                              & 10.20                              & 250.80                             & 87.07                              \\ \bottomrule
        \end{tabular}
    }
    \caption[Detailed performance evaluation for our LLHD simulator.]{Detailed evaluation of our LLHD simulator. The \textit{Total} column indicates the total time required to run the simulation from start to end, as reported by the \texttt{time} command line tool. The \textit{Signals udpate} column indicates the total time required to process drive changes and update the signal values. The \textit{Gather instance arguments} indicates the time required to fetch the arguments to pass to the JIT-compiled unit function when running a unit. The \textit{JIT execution time} indicates the time spent running the JIT-compiled units. Finally, the \textit{Remainder} column reports the difference between the total time and the stages we track, which include launching and destroying the program, and the iteration over the event and wake-up queues.}
    \label{tab:time_detail0}
\end{table}

We recognize our first prototype prioritized correctness over performance, and as such many of the used data structures and approaches are far from being optimal. Due to time constraints we were also not able to bring major changes targeted at improving performance, but we observe that some performance optimizations include some really low-hanging fruits.

In Table \ref{tab:time_detail1} we list the timing results we obtain after just applying some more consideration in the used data structures, such as using LLVM \texttt{SmallVector}s instead of standard vectors where fit, as well as avoiding looking up the converted units' function pointers through the JIT, and instead storing them into the state.

\begin{table}
    \centering
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{llllll}
            \toprule
            \textbf{Design} & \textbf{Total [s]} & \begin{tabular}[c]{@{}l@{}}\textbf{Signals}\\\textbf{update [s]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Gather instance}\\\textbf{arguments [s]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{JIT execution}\\\textbf{time [s]}\end{tabular} & \textbf{Remainder [s]} \\
            \toprule
            Graycode        & 62.47              & 14.10                     & 4.00                       & 23.40                      & 20.97                  \\
            LFSR            & 188.61             & 61.60                     & 14.10                      & 51.00                      & 61.91                  \\
            CDC (strobe)    & 215.18             & 56.20                     & 10.50                      & 70.50                      & 77.98                  \\
            Stream Delayer  & 54.04              & 14.70                     & 2.00                       & 17.20                      & 20.14                  \\
            FIFO Queue      & 52.08              & 14.40                     & 2.00                       & 16.90                      & 18.78                  \\
            LZC             & 63.20              & 14.00                     & 1.70                       & 31.60                      & 15.90                  \\
            RR Arbiter      & 563.83             & 124.50                    & 9.40                       & 264.30                     & 165.63                 \\
            FIR Filter      & 248.55             & 60.60                     & 10.60                      & 97.60                      & 79.75                  \\
            CDC (Gray)      & 164.92             & 47.30                     & 6.10                       & 53.80                      & 57.72                  \\
            Snitch          & 204.61             & 42.00                     & 3.30                       & 89.10                      & 70.21                  \\
            \bottomrule
        \end{tabular}
    }
    \caption{Detailed break down of the simulation timings, after a couple of small adjustments.}
    \label{tab:time_detail1}
\end{table}

With just those smaller considerations, we already observe a performance improvement by an average factor of $4$, showing that our current implementation is far from being optimal.

Inspecting the results from a performance monitoring tool, such as \texttt{perf}, we observe that the majority of the time in our simulator is spent in re-ordering and popping the queue, indicating our event queue structure is not yet optimal. This also affects the time required to execute the units, since driving a signal implies interacting with the event-queue.

We also observe that setting the LLVM code-gen optimization level to $O3$ does not translate to any changes in most of the examples from the LLVM IR code generated with the $O2$ flag. We could also observe the same behavior with the LLHD Blaze simulator. We deduce that the generated code after the optimizations enabled by the $O2$ flag mostly contains memory interaction (either with the state or with signals) and runtime function calls (signal drives) that can generally not be further optimized. Improving on those aspects could further improve the amount of optimization that could be applied to the LLVM IR code we are generating.